{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification with TensorFlow",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rUCmHhHiC6E"
      },
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPnjt9pQiMkt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "11545d8e-d2c3-4462-8cb4-33be2fa97eea"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv \\\n",
        "    -O /tmp/bbc-text.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-02 11:17:18--  https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 74.125.142.128, 74.125.195.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5057493 (4.8M) [text/csv]\n",
            "Saving to: ‘/tmp/bbc-text.csv’\n",
            "\n",
            "/tmp/bbc-text.csv   100%[===================>]   4.82M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-09-02 11:17:18 (106 MB/s) - ‘/tmp/bbc-text.csv’ saved [5057493/5057493]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-tsIqd7iRyA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "84ba60fb-a1ce-4fe2-9dd0-ce2183363c37"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfQXl5zgiV-p"
      },
      "source": [
        "vocab_size = 5000 # make the top list of words (common words)\n",
        "embedding_dim = 64\n",
        "max_length = 200\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = '<OOV>' # OOV = Out of Vocabulary\n",
        "training_portion = .8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpSoSuK5iZ2d"
      },
      "source": [
        "articles = []\n",
        "labels = []\n",
        "\n",
        "with open(\"/tmp/bbc-text.csv\", 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        labels.append(row[0])\n",
        "        article = row[1]\n",
        "        for word in STOPWORDS:\n",
        "            token = ' ' + word + ' '\n",
        "            article = article.replace(token, ' ')\n",
        "            article = article.replace(' ', ' ')\n",
        "        articles.append(article)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcbQt73_icWC"
      },
      "source": [
        "train_size = int(len(articles) * training_portion)\n",
        "\n",
        "train_articles = articles[0: train_size]\n",
        "train_labels = labels[0: train_size]\n",
        "\n",
        "validation_articles = articles[train_size:]\n",
        "validation_labels = labels[train_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHnj8T-eigtW"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_articles)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4DVBXQ5ijh6"
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od1tRKQEikNn"
      },
      "source": [
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORLzkSM_io2N"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_articles)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
        "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtBLGnTWiwWk"
      },
      "source": [
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
        "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me68QmYKjAAg"
      },
      "source": [
        "import tensorflow as tf\n",
        "  \n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "    #tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TUiU9ckjVlj"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX5eExpljY8n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "1809f0c5-1eac-4c95-ff83-5edbc536c160"
      },
      "source": [
        "num_epochs = 10\n",
        "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "56/56 - 12s - loss: 1.5842 - accuracy: 0.2798 - val_loss: 1.3054 - val_accuracy: 0.5551\n",
            "Epoch 2/10\n",
            "56/56 - 11s - loss: 1.2491 - accuracy: 0.4888 - val_loss: 0.8940 - val_accuracy: 0.6831\n",
            "Epoch 3/10\n",
            "56/56 - 11s - loss: 0.6714 - accuracy: 0.7837 - val_loss: 0.4813 - val_accuracy: 0.8764\n",
            "Epoch 4/10\n",
            "56/56 - 11s - loss: 0.3183 - accuracy: 0.8978 - val_loss: 0.4201 - val_accuracy: 0.8472\n",
            "Epoch 5/10\n",
            "56/56 - 11s - loss: 0.5183 - accuracy: 0.8562 - val_loss: 0.5305 - val_accuracy: 0.8404\n",
            "Epoch 6/10\n",
            "56/56 - 11s - loss: 0.1851 - accuracy: 0.9596 - val_loss: 0.1952 - val_accuracy: 0.9483\n",
            "Epoch 7/10\n",
            "56/56 - 11s - loss: 0.0827 - accuracy: 0.9781 - val_loss: 0.1678 - val_accuracy: 0.9416\n",
            "Epoch 8/10\n",
            "56/56 - 11s - loss: 0.0633 - accuracy: 0.9826 - val_loss: 0.1575 - val_accuracy: 0.9596\n",
            "Epoch 9/10\n",
            "56/56 - 11s - loss: 0.0379 - accuracy: 0.9876 - val_loss: 0.1427 - val_accuracy: 0.9551\n",
            "Epoch 10/10\n",
            "56/56 - 11s - loss: 0.0294 - accuracy: 0.9938 - val_loss: 0.1611 - val_accuracy: 0.9551\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}